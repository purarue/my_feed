#!/usr/bin/env python3

import subprocess
import shlex
import itertools
from functools import cache
from pathlib import Path
from typing import List, Any, Dict, Iterator
from urllib.parse import urljoin
from datetime import date

import click
import requests
from logzero import logger
import more_itertools


@cache
def evry_data_dir() -> Path:
    proc = subprocess.run(
        shlex.split("evry location -tag"), stdout=subprocess.PIPE, check=True
    )
    return Path(proc.stdout.decode("utf-8").strip()).parent


def evry_clear(tag: str) -> None:
    target = evry_data_dir() / tag
    if target.exists():
        click.echo(f"removing '{target}'", err=True)
        target.unlink()


BASE = "http://localhost:5100/data/"

Json = Any


@cache
def request_data(params: str, limit: int = 500) -> Json:
    if "&" not in params and "=" not in params:
        params = f"ftype={params}"
    url = urljoin(BASE, f"?offset=0&order_by=when&sort=desc&limit={limit}&" + params)
    logger.info(f"Requesting {url}")
    req = requests.get(url)
    req.raise_for_status()
    return req.json()


def check_albums(**kwargs: str) -> str | None:
    from nextalbums.export import export_data
    from my_feed.sources.nextalbums import _album_id

    recent_albums = sorted(
        (
            a
            for a in export_data()
            if not isinstance(a, Exception) and a.listened_on is not None
        ),
        key=lambda p: p.listened_on or date.min,
        reverse=True,
    )

    # remove 'album_' prefix from albums
    remote_ids = {ll["id"].split("_", maxsplit=1)[1] for ll in request_data("album")}

    for r in map(_album_id, itertools.islice(recent_albums, 50)):
        if r not in remote_ids:
            click.echo(f"Missing {r}", err=True)
            evry_clear("backup_albums")
            return "nextalbums.history"


def check_mpv(**kwargs: str) -> str | None:
    from my.mpv.history_daemon import inputs
    from my_feed.sources.mpv import history

    def _inputs() -> List[Path]:
        # ignore merged files
        return [f for f in reversed(inputs()) if "merged" not in f.name]

    if len(_inputs()) == 0:
        return

    last = more_itertools.first(history(from_paths=_inputs), default=None)
    if last is None:
        return

    remote_ids = {
        ll["id"] for ll in request_data("listen") if ll["id"].startswith("mpv_")
    }
    if len(remote_ids) == 0:
        return

    if last.id not in remote_ids:
        return "mpv.history"


def check_listens(**kwargs: str) -> str | None:
    from listenbrainz_export.export import request_listens

    remote_ids = [ll for ll in request_data("listen") if ll["id"].startswith("listen")]
    if len(remote_ids) == 0:
        logger.info("No remote listens found")
        return

    username = kwargs.get("listenbrainz-username")
    if username is None:
        logger.warning("No username set, use the --listenbrainz-username flag")
        return
    listens = [
        ls
        for ls in request_listens(username=username, pages=1)
        if ls["listened_at"] is not None
    ]
    assert len(listens) > 0, "received no listens from listenbrainz"
    most_recent_epoch = listens[0]["listened_at"]
    if int(most_recent_epoch) > int(remote_ids[0]["when"]):
        evry_clear("backup_listenbrainz_partial")
        return "listens.history"


def check_chess(**kwargs: str) -> str | None:
    from chess_export.chessdotcom.export import (
        safe_request_json,
        get_player_game_archives,
        _user_agent,
    )

    remote_end_times = {int(ll["when"]) for ll in request_data("chess", limit=10)}

    username = kwargs.get("chessdotcom-username")
    if username is None:
        logger.warning("No username set, use the --chessdotcom-username flag")
        return
    chess_last_month_url = get_player_game_archives(username)[-1]
    games_json = safe_request_json(chess_last_month_url, headers=_user_agent())
    if "games" not in games_json:
        logger.warning(f"No games found in JSON: {games_json}")
        return
    games = games_json["games"]
    games.sort(key=lambda g: g["end_time"])
    if len(games) == 0:
        return
    latest_end_time = int(games[-1]["end_time"])

    if latest_end_time not in remote_end_times:
        evry_clear("backup_chess")
        return "chess"


def check_trakt(**kwargs: str) -> str | None:
    from traktexport.export import partial_export

    username = kwargs.get("trakt-username")
    if username is None:
        logger.warning("No username set, use the --trakt-username flag")
        return
    hist_ids = [
        int(data["id"])
        for data in partial_export(username=username, pages=1)["history"]
    ]
    if len(hist_ids) == 0:
        return

    remote_ids = {
        int(data["id"].split("_")[-1])
        for data in request_data("trakt_history_movie,trakt_history_episode", limit=10)
    }
    if len(remote_ids) == 0:
        logger.info("No remote trakt history found")
        return

    if hist_ids[0] not in remote_ids:
        evry_clear("backup_trakt_partial")
        return "trakt.history"


def check_mal(**kwargs: str) -> str | None:
    # this doesn't check MAL for episodes, just for local manual-history episodes I might have logged
    # or updates from running malexport_partial_update manually
    # https://github.com/seanbreckenridge/HPI-personal/blob/master/scripts/malexport_partial_update
    from my_feed.sources.mal import _anime

    remote_ids = [data["id"] for data in request_data("anime,anime_episode", limit=50)]

    if len(remote_ids) == 0:
        logger.info("No remote mal history found")
        return

    local_history = sorted(_anime(), key=lambda a: a.when)
    if len(local_history) == 0:
        logger.info("No local mal history found")
        return

    latest = more_itertools.last(local_history)

    if latest.id not in remote_ids:
        logger.info(f"Latest local anime {latest} not in remote history")
        return "mal.history"


FUNCS = [
    check_albums,
    check_trakt,
    check_chess,
    check_mpv,
    check_listens,
    check_mal,
]


def check(**kwargs: Any) -> Iterator[str]:
    for func in FUNCS:
        try:
            logger.info(f"Checking '{func.__qualname__}'")
            if updated := func(**kwargs):
                yield updated
        except Exception as e:
            logger.exception(f"{func.__qualname__} failed...", exc_info=e)


def _parse_unknown(unknown: List[str]) -> Dict[str, str]:
    kwargs: Dict[str, str] = {}
    for flag, val in more_itertools.sliced(unknown, 2):
        kwargs[flag.lstrip("-").strip()] = val
    return kwargs


@click.command(
    context_settings=dict(
        ignore_unknown_options=True,
        allow_extra_args=True,
    )
)
@click.option("--remote-base", help="Base of remote feed API", type=str, default=BASE)
@click.argument("KWARGS", nargs=-1, type=click.UNPROCESSED)
def main(remote_base: str, kwargs: Any) -> None:
    global BASE
    BASE = remote_base
    kw = _parse_unknown(kwargs)
    expired = [f for f in check(**kw) if f is not None]
    if expired:
        click.echo(",".join(expired))
    else:
        logger.info("No feed items are out of date")


if __name__ == "__main__":
    main()
