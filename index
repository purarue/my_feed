#!/usr/bin/env zsh
# if FEED_REINDEX=1 ./index , this removes all the data from the remote database and re-builds it
# the remote database and re-builds it

cd "$(realpath "$(dirname "${BASH_SOURCE[0]}")")" || exit $?
with_secrets_script="${HOME}/.local/scripts/generic/with-secrets"
if [[ -f "$with_secrets_script" ]]; then
	echo 'Sourcing secrets file...' >&2
	source "$with_secrets_script" || true
else
	echo 'Missing with-secrets script' >&2
	exit 1
fi

set -e
set -o pipefail

declare -a CURL_AUTH_OPTS=()
declare SSH_TARGET
# setup Host in your ~/.ssh/config file
SSH_TARGET="${SSH_TARGET:-vultr}"

auth_curl() {
	curl "${CURL_AUTH_OPTS[@]}" "$@"
}

[[ -n "$FEED_REINDEX" ]] && printf 'Re'
echo "Indexing..."

# https://github.com/seanbreckenridge/wait-for-internet
wait-for-internet -q --timeout 30 || exit 0

[[ -n "$MY_FEED_SECRET" ]] && CURL_AUTH_OPTS=("-H" "token:$MY_FEED_SECRET")

# temporary dir for new data
TMPDIR="$(mktemp -d)"
# https://github.com/seanbreckenridge/seanb-utils/blob/main/shellscripts/epoch
JSON="${TMPDIR}/$(epoch).json" || exit $?

# warm tz cache in case its expired, use flock in case something else is already running doctor
flock ~/.local/tz-lock hpi doctor -S my.time.tz.via_location

# run an index
INDEX_ARGS=()
# if we have a list of blurred images, pass it to the indexer
BLURRED_IMAGES="${HPIDATA}/feed_blurred_images.txt"
if [[ -f "$BLURRED_IMAGES" ]]; then
	# if we have a list of blurred images, pass it to the indexer
	INDEX_ARGS+=("-B" "$BLURRED_IMAGES")
fi
if [[ -z "$FEED_REINDEX" ]]; then
	# if were not re-indexing, fetch the list of IDs we've already indexed from the server
	# and pass it to the indexer, so it can skip uploading those
	curl "${CURL_AUTH_OPTS[@]}" -sL 'https://sean.fish/feed_api/data/ids' >"${TMPDIR}/ids.json" || exit $?
	INDEX_ARGS+=("-E" "${TMPDIR}/ids.json")
	# stuff to ignore here which takes a long time and/or doesn't commonly change
	# can be pushed just when doing a re-index
	export MY_FEED_EXCLUDE_SOURCES='mal.deleted,games.grouvee,games.game_center,facebook_spotify_listens,games.osrs'
else
	# running a re-index, so update the approved IDs for computing deleted anime entry data
	# https://github.com/seanbreckenridge/malexport/#recover_deleted
	python3 -m malexport recover-deleted approved-update
	export RUNELITE_PHOTOS_PREFIX='https://sean.fish/' # set prefix for indexer
	vps_sync_osrs_images || exit $?
fi

# write count to file
INDEX_ARGS+=("-C" "${TMPDIR}/count.txt")
flock ~/.local/feed-lock my_feed index "${INDEX_ARGS[@]}" "$JSON" || exit $?

COUNT="$(cat "${TMPDIR}/count.txt")"

# if ids.json/count.txt file exists, delete it
[[ -f "${TMPDIR}/ids.json" ]] && command rm -fv "${TMPDIR}/ids.json"
[[ -f "${TMPDIR}/count.txt" ]] && command rm -fv "${TMPDIR}/count.txt"

# if the json file is empty, don't bother uploading
if [[ "$COUNT" -eq 0 ]]; then
	echo 'No new data, exiting' >&2
	# delete temp file
	command rm -f "${JSON}"
	exit 0
fi

wait-for-internet -q --timeout "${WFI_TIMEOUT:-10}" || exit 0

# delete remote json files if we want to reset
[[ -n "$FEED_REINDEX" ]] && auth_curl -sL 'https://sean.fish/feed_api/clear-data-dir'

# copy up to the server
flock ~/.local/feed-sync-lock scp "${JSON}" "${SSH_TARGET}":~/code/my_feed/backend/data

# delete temp file
command rm -f "${JSON}"
rmdir "${TMPDIR}"

url="https://sean.fish/feed_api/check"
if [[ -n "$FEED_REINDEX" ]]; then
	url="https://sean.fish/feed_api/recheck"
	echo 'Running reindex...'
else
	echo 'Running update...'
fi
curl "${CURL_AUTH_OPTS[@]}" -sL "$url" | jq 'to_entries[] | select(.value != null) | "\(.key): \(.value)"' -r
